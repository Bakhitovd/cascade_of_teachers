Alright, here’s a **technical spec you can hand to another engineer** to implement and run the teacher cascade over your existing manifests.

I’ll assume they know Python, PyTorch, HuggingFace, and basic GPU stuff.

---

## 0. Goal

Implement an **offline teacher cascade** that reads your existing manifests:

* **EN**: 1,800 h People’s Speech (CSV manifest)
* **RU**: 1,050 h Golos (CSV manifest)

and produces **parallel S2S training data** for a RU↔EN student model:

* `parallel/ruen/*` = RU → EN direction (source: Golos)
* (optionally later) `parallel/enru/*` = EN → RU direction (source: People’s Speech)

Each example contains:

* `src.wav` – original speech (RU or EN)
* `src.txt` – ASR transcript (Whisper)
* `tgt.txt` – translation (M2M-100, later GPT-4o)
* `tgt.wav` – TTS/VC speech in target language (OpenVoice)
* `spk.npy` – speaker embedding (ECAPA)
* `tokens.npy` – EnCodec tokens of `tgt.wav`
* `meta.json` – metadata, including original manifest row and pipeline stats

---

## 1. Input & Output Specs

### 1.1 Input manifests (already exist)

**English (People’s Speech)** – CSV columns (from your example):

```text
id,lang,dataset,split,tier,tier_rank,audio_path,duration_s,text,...
```

* `lang` = `"en"`
* `audio_path` = e.g. `keep\en\gold_plus_clean_train\0728...eda7.wav`
* `text` = English transcript (we may or may not trust it; Whisper will re-ASR if needed)

**Russian (Golos)** – CSV columns (from your example):

```text
id,lang,dataset,split,audio_path,duration_s,text,...,tier,tier_rank,...
```

* `lang` = `"ru"`
* `audio_path` = e.g. `keep\ru\golos\train\crowd\6\712a8...f1b.wav`
* `text` = Russian text (again, Whisper will re-ASR to normalize)
* `tier` = `"Gold"` (already filtered)

Paths use `\`; treat them as **relative to a configurable data root**.

---

### 1.2 Output directory layout

Engineer should implement exactly this:

```text
data/
  parallel/
    ruen/
      <id>/
        src.wav
        src.txt
        tgt.wav
        tgt.txt
        spk.npy
        tokens.npy
        meta.json
        DONE          # marker file

    enru/            # (optional, same structure for EN→RU later)
```

EnCodec tokens and speakers can be stored either:

* per-sample in `parallel/.../<id>/tokens.npy`, `spk.npy`,
  or
* in separate `tokens/ruen/` and `speakers/ru/` dirs.

Pick **one** and be consistent; below I assume all inside `parallel/.../<id>/`.

---

## 2. Environment & Models

Engineer should set up a dedicated env, e.g.:

```bash
conda create -n s2s_teachers python=3.10 -y
conda activate s2s_teachers

pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu122
pip install transformers sentencepiece accelerate
pip install pandas numpy tqdm pyyaml soundfile librosa
pip install encodec
# ECAPA speaker encoder implementation (e.g. speechbrain) 
pip install speechbrain
```

**Models to use:**

1. **Whisper Large V3**

   * HF model id: `openai/whisper-large-v3` (or whichever mirror you use)
   * Runs on GPU

2. **M2M-100**

   * HF model id: `facebook/m2m100_1.2B`
   * Direction: `ru→en` and (later) `en→ru`

3. **OpenVoice**

   * From its Git repo (engineer should install per README)
   * Used as multilingual TTS/VC: text + speaker embedding → waveform

4. **EnCodec 24 kHz 6 kbps**

   * HF-ish: `facebook/encodec_24khz`
   * Produces discrete codes

5. **ECAPA-TDNN speaker encoder**

   * From `speechbrain` (`speechbrain/spkrec-ecapa-voxceleb`)

---

## 3. Configuration

Create a `config.yaml` like:

```yaml
data_root: "/mnt/data"         # base dir that contains 'keep/'
output_root: "/mnt/data/s2s"   # where 'data/parallel' will live

manifests:
  ru: "/mnt/data/manifests/golos_filtered.csv"
  en: "/mnt/data/manifests/peoples_speech_filtered.csv"

pipeline:
  min_duration: 2.0     # seconds
  max_duration: 15.0
  direction: "ruen"     # for now; later "enru"
  use_gpt4o: false      # for translation; starts with M2M-100

compute:
  num_workers: 4        # CPU loader threads
  device: "cuda:0"
  log_interval: 10
```

Engineer should read this config and drive everything via a CLI:

```bash
python run_teacher_cascade.py --config config.yaml --lang ru --direction ruen
```

---

## 4. High-Level Pipeline Logic

### 4.1 Control flow per language/direction

For **RU → EN**:

1. Load RU manifest (Golos).
2. Filter:

   * `lang == 'ru'`
   * `tier == 'Gold'` (or similar)
   * `min_duration <= duration_s <= max_duration`
3. Shuffle or shard for parallel processing.
4. For each row:

   * Compute sample ID → sanitize to filesystem-safe string.
   * Output directory: `output_root/data/parallel/ruen/<id>/`
   * If `<id>/DONE` exists → skip (already processed).
   * Run the 5 teacher steps (below).
   * Save outputs.
   * Touch `DONE`.

For **EN → RU** (later), same pattern, but translation direction reversed.

---

## 5. Detailed Steps per Sample (RU → EN)

All heavy models run **on GPU**, but the pipeline should be structured so that:

* audio loading is I/O threaded,
* model calls are sequential per sample (simpler and robust).

### Step 0 – Load audio

```python
wav, sr = load_audio(abs_path, target_sr=16000)
duration = len(wav) / target_sr
# optional: check duration against manifest, drop if mismatch > 10%
```

### Step 1 – Whisper ASR (source transcript)

Goal: produce `src.txt` (normalized RU text).

```python
ru_text = whisper_transcribe(wav, language="ru")
```

Implementation skeleton:

```python
from transformers import WhisperProcessor, WhisperForConditionalGeneration

processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3")
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3").to(device)

def whisper_transcribe(wav, language):
    input_features = processor(
        wav, sampling_rate=16000, return_tensors="pt"
    ).input_features.to(device)
    forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task="transcribe")
    with torch.no_grad():
        pred_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)
    text = processor.batch_decode(pred_ids, skip_special_tokens=True)[0].strip()
    return text
```

Save as:

```text
parallel/ruen/<id>/src.wav   # copy or symlink original
parallel/ruen/<id>/src.txt
```

> If you trust original `text` in Golos, you can optionally compare; but Whisper transcript should be the canonical source.

---

### Step 2 – M2M-100 Translation (RU → EN)

Goal: produce `tgt.txt` (EN translation).

```python
en_text = translate_ru_en(ru_text)
```

Implementation skeleton:

```python
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_1.2B")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_1.2B").to(device)

def translate_ru_en(text: str) -> str:
    tokenizer.src_lang = "ru"
    encoded = tokenizer(text, return_tensors="pt").to(device)
    generated = model.generate(**encoded, forced_bos_token_id=tokenizer.get_lang_id("en"))
    out = tokenizer.batch_decode(generated, skip_special_tokens=True)[0].strip()
    return out
```

Later, this function can be swapped for GPT-4o-mini calls with the same signature.

Save as:

```text
parallel/ruen/<id>/tgt.txt
```

---

### Step 3 – Speaker embedding (ECAPA)

Goal: produce `spk.npy` (speaker vector from **source** audio).

```python
spk_emb = ecapa_embed(wav)  # numpy array shape (256,) or (512,)
```

Implementation sketch (using speechbrain):

```python
import torch
import numpy as np
from speechbrain.pretrained import SpeakerRecognition

spkrec = SpeakerRecognition.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    savedir="pretrained_models/spkrec-ecapa-voxceleb",
)

def ecapa_embed(wav_np: np.ndarray) -> np.ndarray:
    wav_tensor = torch.from_numpy(wav_np).float().unsqueeze(0)
    with torch.no_grad():
        emb = spkrec.encode_batch(wav_tensor).squeeze(0).squeeze(0)
    return emb.cpu().numpy()
```

Save as:

```text
parallel/ruen/<id>/spk.npy
```

---

### Step 4 – OpenVoice TTS/VC (EN speech in same voice)

Goal: produce `tgt.wav` (EN speech in the **same voice**).

This is the most implementation-specific part because OpenVoice’s exact API depends on how it’s wired. Engineer should:

* Initialize OpenVoice model once on GPU.
* Provide:

  * `en_text` as input text.
  * `spk_emb` as conditioning vector (or provide `src.wav` as reference if the model works in VC mode).

Pseudocode:

```python
tgt_wav, tgt_sr = openvoice_synthesize(text=en_text, speaker_embedding=spk_emb)
```

Save as 24 kHz mono WAV:

```text
parallel/ruen/<id>/tgt.wav
```

If OpenVoice natively does VC from reference audio instead of emb, change step:

* Pass `src.wav` as reference for timbre,
* Still compute `spk_emb` for the student supervision.

---

### Step 5 – EnCodec tokens from target speech

Goal: produce `tokens.npy` (training targets for student).

```python
tokens = encodec_encode(tgt_wav, tgt_sr)  # e.g. shape (num_codebooks, num_frames)
```

Implementation sketch:

```python
from encodec import EncodecModel
from encodec.utils import convert_audio

model = EncodecModel.encodec_model_24khz()
model.set_target_bandwidth(6.0)  # kbps
model.to(device)

def encodec_encode(wav_np: np.ndarray, sr: int) -> np.ndarray:
    wav_tensor = torch.from_numpy(wav_np).float().unsqueeze(0).to(device)
    wav_24k = convert_audio(wav_tensor, sr, 24000, 1)
    with torch.no_grad():
        encoded_frames = model.encode(wav_24k)
    # encoded_frames is list of length T with dicts, or model-dependent; flatten to [B,num_q,T]
    codes = torch.cat([f.codes for f in encoded_frames], dim=-1)  # [B, num_q, T]
    return codes.squeeze(0).cpu().numpy()
```

Save as:

```text
parallel/ruen/<id>/tokens.npy
```

---

### Step 6 – Metadata & marker

Create `meta.json` with:

* Original manifest fields (duration, dataset, split, tier, etc.)
* Pipeline outputs:

  * `src_lang`, `tgt_lang`
  * `src_duration`, `tgt_duration`
  * ASR text length, translation length
  * Some simple quality flags (e.g. empty text, too short tokens → mark as bad)

Example:

```json
{
  "id": "712a838436ca553175990ce1f1bf5f1b",
  "src_lang": "ru",
  "tgt_lang": "en",
  "dataset": "golos",
  "split": "train",
  "src_duration_s": 5.024,
  "tgt_duration_s": 5.120,
  "src_text_len": 33,
  "tgt_text_len": 48,
  "status": "ok"
}
```

Write a `DONE` marker file after all files are written:

```text
parallel/ruen/<id>/DONE
```

So the script can be restarted and will skip completed ids.

---

## 6. Orchestration Script (`run_teacher_cascade.py`)

Engineer should implement:

```bash
python run_teacher_cascade.py \
  --config config.yaml \
  --lang ru \
  --direction ruen \
  --shard-idx 0 \
  --num-shards 4 \
  --device cuda:0
```

Core logic:

1. Load `config.yaml`.
2. Load manifest for requested `lang`.
3. Apply filtering + sharding:

   * `rows = filtered_rows[shard_idx :: num_shards]`
4. For each row:

   * Build output path; skip if `DONE` exists.
   * Try full pipeline in `try/except`.
   * On error: log row id + exception to `errors_<shard_idx>.log`, continue.
5. Periodically log progress (`every N samples`).

Use `tqdm` over rows for simple progress.

---

## 7. Performance Notes (Your PC)

* Use **single GPU per process**.

* To utilize both 3090 and A4000:

  * Run two shards simultaneously:

    ```bash
    CUDA_VISIBLE_DEVICES=0 python run_teacher_cascade.py --shard-idx 0 --num-shards 2 ...
    CUDA_VISIBLE_DEVICES=1 python run_teacher_cascade.py --shard-idx 1 --num-shards 2 ...
    ```

* Keep batch size = 1 per model; this is I/O bound anyway.

* Start with **small subset** (e.g. 1000 rows) to validate full pipeline end-to-end.

---

That’s the spec an engineer can implement from start to finish.

If you want next, I can turn this into an **actual `run_teacher_cascade.py` skeleton** (with argument parsing and function structure) so they have concrete code shape to fill in.
